---
title: "MATH3091: Statistical Modelling II"
subtitle: "Problem Sheet 5 (Solution)"
format: pdf
header-includes:
  - \usepackage{xcolor}
  - \definecolor{solutionblue}{RGB}{0, 0, 255}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. The time to failure ($Y$) of a certain type of electrical component is thought to follow an exponential distribution, with probability density of the form
$$
f_Y(y;\lambda) = \lambda \exp\left(-\lambda y\right), \quad
y>0;\quad\lambda>0\,.
$$
It is believed that the failure rate of a component $\lambda$ is related to its electrical resistance ($x$) by the relationship
$$
\lambda = \beta_1+\beta_2 x\,.
$$
Suppose that $y_1,\cdots ,y_n$  are observations of the times to failure, $Y_1,\cdots,Y_n$ for $n$ such components with corresponding
resistances $x_1,\cdots,x_n$.

   a. Write down the likelihood in terms of $\beta_1$ and $\beta_2$ and hence derive a pair of simultaneous equations, the solutions of which are the maximum likelihood estimates.

   b. Calculate the observed and expected information matrices. Are the Newton-Raphson and the Fisher scoring methods identical
for this problem? Justify your answer.

\color{solutionblue}
Solution:

a. The likelihood is
$$
\mathcal{L}( \beta) = \prod_{i=1}^n \lambda_i( \beta) e^{-\lambda_i( \beta) y_i}\,.
$$
The log-likelihood is
$$
\ell( \beta) = \sum_{i=1}^n \log \lambda_i( \beta) - \sum_{i=1}^n \lambda_i( \beta) y_i\,.
$$
Differentiating with respect to $\beta_j$, $j = 1, 2$, gives
$$
\frac{\partial}{\partial \beta_j} \ell( \beta) = \sum_{i=1}^n \frac{1}{\lambda_i( \beta)} 
      \frac{\partial \lambda_i}{\partial \beta_j} - \sum_{i=1}^n \frac{\partial \lambda_i}{\partial \beta_j} y_i\,,
$$
where
$$
\frac{\partial \lambda_i}{\partial \beta_1} = 1; \qquad \frac{\partial \lambda_i}{\partial \beta_2} = x_i\,.
$$
So
$$
\frac{\partial}{\partial \beta_1} \ell( \beta) = \sum_{i=1}^n \frac{1}{\lambda_i( \beta)} - \sum_{i=1}^n y_i\,,
$$
and
$$
\frac{\partial}{\partial \beta_2} \ell( \beta) = \sum_{i=1}^n \frac{x_i}{\lambda_i( \beta)}  - \sum_{i=1}^n x_i y_i\,.
$$
So the MLE $\hat{\beta} = (\hat \beta_1, \hat \beta_2)^{\top}$ satisfies
$$
\sum_{i=1}^n \frac{1}{\lambda_i(\hat{ \beta})} = \sum_{i=1}^n y_i; \qquad
      \sum_{i=1}^n \frac{x_i}{\lambda_i(\hat{ \beta})}  = \sum_{i=1}^n x_i y_i\,,
$$
or
$$
\sum_{i=1}^n \frac{1}{\hat \beta_1 + \hat \beta_2 x_i} = \sum_{i=1}^n y_i; 
      \qquad\sum_{i=1}^n \frac{x_i}{\hat \beta_1 + \hat \beta_2 x_i}  = \sum_{i=1}^n x_i y_i\,.
$$
      
b. We have 
\begin{align*}
      H_{jk}( \beta) &= \frac{\partial^2}{\partial \beta_j \partial \beta_k} \ell( \beta) \\
      &=  \sum_{i=1}^n \frac{\partial}{\partial \beta_k} \left(\frac{1}{\lambda_i( \beta)} \frac{\partial \lambda_i}{\partial \beta_j} 
      - \frac{\partial \lambda_i}{\partial \beta_j} y_i\right) \\
      &= \sum_{i=1}^n \frac{-1}{\lambda_i( \beta)^2}\frac{\partial \lambda_i}{\partial \beta_j}\frac{\partial \lambda_i}{\partial \beta_k} 
      + \frac{1}{\lambda_i( \beta)}\frac{\partial^2 \lambda_i}{\partial \beta_j \partial \beta_k} 
      - \frac{\partial^2 \lambda_i}{\partial \beta_j \partial \beta_k} y_i \\
      &= - \sum_{i=1}^n \frac{1}{\lambda_i( \beta)^2}\frac{\partial \lambda_i}{\partial \beta_j}\frac{\partial \lambda_i}{\partial \beta_k} 
\end{align*}
as $\frac{\partial^2 \lambda_i}{\partial \beta_j \partial \beta_k} = 0$.

So the observed information is
$$
-H( \beta) = \begin{pmatrix}
\sum_{i=1}^n \frac{1}{(\beta_1 + \beta_2 x_i)^2} &  \sum_{i=1}^n \frac{x_i}{(\beta_1 + \beta_2 x_i)^2} \\
\sum_{i=1}^n \frac{x_i}{(\beta_1 + \beta_2 x_i)^2} & \sum_{i=1}^n \frac{x_i^2}{(\beta_1 + \beta_2 x_i)^2}
\end{pmatrix}\,,
$$
and the Fisher information matrix $\mathcal{I}(\beta) = -H(\beta)$, as the observed information matrix does not depend on $y$. The Newton-Raphson and Fisher scoring methods will be identical for this problem, because the observed information matrix and the Fisher information matrix are identical.

\color{black}

2. Suppose $Y_i \sim \text{Geometric}(p_i)$, the geometric distribution as studied in Question 2 of Problem Sheet 4. We want to model how $p_i$ depends on explanatory variables $\boldsymbol{x}_i$.

   a. Assuming a GLM with canonical link function, write down a formula for $p_i$ in terms of $\boldsymbol{x}_i$. Is this a sensible model?

   a. Suppose instead that 
   $$
   \text{logit} (p_i) = \log \frac{p_i}{1 - p_i} =  \mathbf{x}_i^{\top} \boldsymbol{\beta}\,.
   $$
   Show that this is a GLM with a non-canonical link function, and write down the link function corresponding to this model.
   
   b. Derive an expression for the scaled deviance for this model, writing $\hat \mu_i$ for the estimate of $\mu_i = \mathbb{E}(Y_i)$ under the model from part (b). Write an expression for $\hat \mu_i$ in terms of $\hat{\boldsymbol{\beta}}$, the MLE of $\boldsymbol{\beta}$.


\color{solutionblue}
Solution:

a. With the canonical link, we always have
$$
\theta_i = \eta_i = {x}_i^{\top} {\beta},
$$
for the canonical parameter $\theta_i$. In this case, from Question 2 of Problem Sheet 4, we have $\theta_i = \log(1 - p_i)$, or $p_i = 1 - \exp\{\theta_i\}$, so
$$
p_i = 1 - \exp\{{x}_i^{\top} {\beta}\}\,.
$$
This is not sensible, as ${x}_i^{\top}{\beta}$ could take any real value, so $1 - \exp\{{x}_i^T {\beta}\}$ could take any value in $(-\infty, 1)$, but we want $p_i \in (0, 1)$.

b. We have $p_i = \mu_i^{-1}$, and $\mu_i = g^{-1}(\eta_i)$, so
$$
p_i = \frac{1}{g^{-1}(\eta_i)}\,.
$$
So we need to choose $g(\cdot)$ such that
$$
\frac{1}{g^{-1}(\eta)} = \text{logit}^{-1}(\eta) = \frac{\exp(\eta)}{1 + \exp(\eta)}\,.
$$
This means that
$$
g^{-1}(\eta) = 1 + \exp(-\eta),
$$
so inverting this gives the required link function
$$
g(\mu) = -\log(\mu - 1)\,.
$$

c. From Problem Sheet 4, Question 2, we have
$$
b(\theta) = -\log(e^{-\theta} - 1)\,, \qquad \mu_i = b'(\theta_i) = \frac{1}{1 - e^\theta_i}\,,
\qquad \theta_i = \log \frac{\mu_i - 1}{\mu_i}\,.
$$
So
$$
b(\theta_i) =  -\log(e^{-\theta_i} - 1) = -\log \left( \frac{\mu_i}{\mu_i - 1} - 1 \right) 
= -\log \left(\frac{1}{\mu_i - 1}\right) = \log(\mu_i - 1)\,.
$$
The scaled deviance is
\begin{align*}
      L_{0s} &= 2\sum_{i=1}^n {y_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i] -[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]}\\
      & = 2\sum_{i=1}^n{y_i\left[\log \frac{\hat \mu_i^{(s)} - 1}{\hat \mu_i^{(s)}}
      - \log \frac{\hat \mu_i^{(0)} - 1}{\hat \mu_i^{(0)}}\right] -\left[\log(\hat\mu_i^{(s)} - 1) - \log(\hat\mu_i^{(0)} - 1) \right]}\,.
\end{align*}

\color{black}

3. We return to the `beetle` data studied in Computer Lab 5, with observations on $n = 8$ groups of beetles. There we considered the model:

```{r, include = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
```

```{r}
beetle_glm <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                  weights = exposed)
```

We could have also considered a model with quadratic dependence on `dose`
```{r}
beetle_glm_quad <- glm(prop_killed ~ dose + I(dose^2), data = beetle, 
                       family = binomial, weights = exposed)
```

   a. Write down mathematical expressions for the two models. Show that `beetle_glm` is nested with `beetle_glm_quad`, and write down the null hypothesis $H_0$ and the alternative hypothesis $H_1$ you would use for comparing the models.
   
   b. Consider the following output of a `summary()` call. What is the scaled deviance for `beetle_glm`? 
```{r}
summary(beetle_glm)
```
    
   c. The scaled deviance for `beetle_glm_quad` is $3.1949$. Calculate the log likelihood ratio test statistic $L_{01}$ for testing $H_0$ against $H_1$. Under $H_0$, what is the distribution of this statistic? Hence conduct a hypothesis test of $H_0$ against $H_1$, and make a conclusion about which model you prefer. 
   
\color{solutionblue}
Solution:

a. Let $Y_i$ be the number of beetles killed and $x_i$ for the dose in group $i$. In both cases, we have $Y_i \sim \text{binomial}(n_i, p_i)$, where $\text{logit}(p_i) = \eta_i$ and $n_i$ is the number of beetles exposed in the $i$-th group. In `beetle_glm`, we have
$$
\eta_i = \beta_1 + \beta_2 x_i\,.
$$
In `beetle_glm_quad`, we have
$$
\eta_i = \beta_1 + \beta_2 x_i + \beta_3 x_i^2\,.
$$

We can compare these models by testing $H_0: \beta_3 = 0$ against $H_1: \text{``$\beta_3$ is unrestricted''}$.

b. The scaled deviance for `beetle_glm` is 11.232.

c. We have $L_{01} = L_{0s} - L_{1s}$, where $L_{0s}$ is the scaled deviance under $H_0$ (`mod_glm`), so $L_{0s} = 11.232$, and $L_{1s}$ is the scaled deviance under $H_1$ (`mod_glm_quad`), so $L_{1s} = 3.1949$. So $L_{01} = 11.232 - 3.1949 = 8.04$.

Under $H_0$, $L_{01} \sim \chi^2_1$, as $p - q = 3 - 2 = 1$. So we should reject $H_0$ if $L_{01}$ is greater than the $95\%$ point of the $\chi^2_1$ distribution, or
```{r}
qchisq(0.95, df = 1) 
```
Since $8.04 > 3.84$, we reject $H_0$, and prefer `beetle_glm_quad` to `beetle_glm`.

We could do this test in `R` with
```{r}
anova(beetle_glm, beetle_glm_quad, test = "LRT")
```

\color{black}
